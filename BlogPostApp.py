# -*- coding: utf-8 -*-
"""Untitled4 (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PwaMzW5yR1DEay7gz4P-Xtf6eeS-52Zq
"""

!pip install transformers

!pip install gradio

!pip install -q -U peft bitsandbytes

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "gamzeyy/mistral-7b-dolly"

config = PeftConfig.from_pretrained(peft_model_id)

from transformers import BitsAndBytesConfig

!pip install accelerate bitsandbytes

model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    return_dict=True,
    load_in_4bit=True, #agırlıkları 4 bit


)

#Tokenezier yükleme
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,
                                          padding_side = "right", #sagdaki boşluk doldurmak için
                                          add_eos_token = True)   #dizinin sonuna metin bittignii gösteren özel token eklenir
tokenizer.pad_token = tokenizer.eos_token

fine_tuned_model = PeftModel.from_pretrained(model, peft_model_id) #basemodel + egitilen modeli birleştir

from transformers import pipeline, logging

logging.set_verbosity(logging.CRITICAL)

pipe = pipeline(
    task="text-generation",
    model=fine_tuned_model,
    tokenizer=tokenizer,
    eos_token_id=model.config.eos_token_id,
    max_new_tokens=100)

prompt = """
What is a Python?  Here is some context: Python is a high-level, general-purpose programming language.
"""
pipe = pipeline(task="text-generation",
                model=fine_tuned_model,
                tokenizer=tokenizer,
                eos_token_id=model.config.eos_token_id,
                max_new_tokens=100)

result = pipe(f"<s>[INST] {prompt} [/INST]") #prompt için özel tokenler
generated = result[0]['generated_text']
print(generated[generated.find('[/INST]')+8:])

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!pip uninstall typing-extensions
!pip install -U typing-extensions

!pip install gradio transformers

import gradio as gr


def generate_summary(prompt):
    pipe = pipeline(task="text-generation",
                    model=fine_tuned_model,
                    tokenizer=tokenizer,
                    eos_token_id=fine_tuned_model.config.eos_token_id,
                    max_new_tokens=100)

    result = pipe(f"<s>[INST] {prompt} [/INST]")
    generated_text = result[0]['generated_text']
    return generated_text[generated_text.find('[/INST]')+8:]

iface = gr.Interface(
    fn=generate_summary,
    inputs=gr.Textbox(placeholder="Blog konusu giriniz...", lines=5),
    outputs=gr.Textbox(),
    title="BLOG POST ÜRET",
    live=True,
    css='div {margin-left: auto; margin-right: auto; width: 100%;\
            background: linear-gradient(90deg, rgba(131,58,180,1) 0%, rgba(253,29,29,1) 50%, rgba(252,176,69,1) 100%);}'

)

iface.launch(share=True)

